{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fasttext CNN Individual.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMP2ZEEbbM/TQFFIOu3LX7i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZKqD6xGVQFO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "22d890e8-d7c0-41c6-fe34-58a22eaa4d9d"
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "from gensim.models.fasttext import FastText\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.14.63)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.63)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAyOIYK8MVlQ"
      },
      "source": [
        "import pandas as pd\n",
        "intents =['update_personal_details','repos_benefits','get_lc_form','sl_development_bond_benefits','life_insurance_limit','marriage_claim','account_currency','foreign_account_lkr_withdrawal','joint_account_details','SLBFE_info','selan_sure_info','FCAISPE_required_docs','resident_foreign_account_info']\n",
        "def load_dataset(filename):\n",
        "  df = pd.read_csv(filename, encoding = \"utf-8\",\n",
        "       names = [\"Sentence\", \"Intent\"])\n",
        "  df = df.loc[df['Intent'].isin(intents)]\n",
        "  intent = df[\"Intent\"]\n",
        "  unique_intent = list(set(intent))\n",
        "  sentences = list(df[\"Sentence\"])\n",
        "  \n",
        "  return (intent, unique_intent, sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAwcRfQMMYTm"
      },
      "source": [
        "intent, unique_intent, sentences = load_dataset(\"/content/bank.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-NL_yHEMY6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d38dea3-7986-48dd-cda2-82ced5dab7d3"
      },
      "source": [
        "nltk.download(\"punkt\")\n",
        "def cleaning(sentences):\n",
        "  words = [] \n",
        "  for s in sentences:\n",
        "    w = word_tokenize(s)\n",
        "    words.append([i for i in w])     \n",
        "  return words  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF4-4KsDMbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6930ce00-9462-4f1c-f867-0178c2f0a53c"
      },
      "source": [
        "cleaned_words = cleaning(sentences)\n",
        "print(len(cleaned_words))\n",
        "print(cleaned_words[:2])  \n",
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "345\n",
            "[['செலான்', 'வங்கியில்', 'கணக்கை', 'ஆரம்பிக்க', 'பயன்படுத்தக்கூடிய', 'நாணயங்கள்', 'எவை', '?'], ['செலான்', 'வங்கியில்', 'கணக்கை', 'ஆரம்பிக்க', 'பயன்படுத்தக்கூடிய', 'நாணயங்கள்', '.']]\n",
            "345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWS8N6MjMfD1"
      },
      "source": [
        "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
        "  token = Tokenizer(filters = filters)\n",
        "  token.fit_on_texts(words)\n",
        "  return token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAw8tlKMh-p"
      },
      "source": [
        "def max_length(words):\n",
        "  return(len(max(words, key = len)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eet3J5TvMmTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc96687b-b47f-48c5-d9ca-e17e70dac91c"
      },
      "source": [
        "word_tokenizer = create_tokenizer(cleaned_words)\n",
        "vocab_size = len(word_tokenizer.word_index) + 1\n",
        "max_length = max_length(cleaned_words)\n",
        "\n",
        "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size = 326 and Maximum length = 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXT_jM84N13J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bcc8e40-77db-4083-da62-c0988511f47c"
      },
      "source": [
        "print(cleaned_words[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['செலான்', 'வங்கியில்', 'கணக்கை', 'ஆரம்பிக்க', 'பயன்படுத்தக்கூடிய', 'நாணயங்கள்', 'எவை', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrb8TsTfO0DP"
      },
      "source": [
        "embedding_size = 300\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uyFxBqmO5c2"
      },
      "source": [
        "ft_model = FastText(cleaned_words,\n",
        "                      size=embedding_size,\n",
        "                      window=window_size,\n",
        "                      min_count=min_word,\n",
        "                      sample=down_sampling,\n",
        "                      sg=1,\n",
        "                      iter=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZUsBvCePQ9W"
      },
      "source": [
        "print(ft_model.wv['2in1'])\n",
        "word_tokenizer.word_index.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUUD8QCLTkdH"
      },
      "source": [
        "word_index =word_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q5nRUj8PZdj"
      },
      "source": [
        "#embedding matrix\n",
        "\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = 326\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, embedding_size ))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = ft_model.wv[word]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbhDIPSBQ83H"
      },
      "source": [
        "print(\"sample words not found: \", words_not_found)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrEE6B2HaP6N"
      },
      "source": [
        "def encoding_doc(token, words):\n",
        "  return(token.texts_to_sequences(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36h8ZzxoZR-g"
      },
      "source": [
        "#tokenizer with filter changed\n",
        "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
        "encoded_output = encoding_doc(output_tokenizer, intent)\n",
        "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
        "encoded_output.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt6k-NT6ZmYf"
      },
      "source": [
        "def one_hot(encode):\n",
        "  o = OneHotEncoder(sparse = False)\n",
        "  return(o.fit_transform(encode))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDhMNArpaXcy"
      },
      "source": [
        "output_one_hot = one_hot(encoded_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQluniMmcElY"
      },
      "source": [
        "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywwncv2OcI6h"
      },
      "source": [
        "def padding_doc(encoded_doc, max_length):\n",
        "  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JQgYtgwcJZz"
      },
      "source": [
        "padded_doc = padding_doc(encoded_doc, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYnTtwOGankF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
        "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
        "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65dVA7CeBtf"
      },
      "source": [
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "batch_size = 256 \n",
        "num_epochs = 8 \n",
        "\n",
        "#model parameters\n",
        "num_filters = 64 \n",
        "#embed_dim = 300 \n",
        "weight_decay = 1e-4\n",
        "\n",
        "num_classes = 13\n",
        "print(\"training CNN ...\")\n",
        "model = Sequential()\n",
        "model.add(Embedding(nb_words, embedding_size, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Conv1D(num_filters, 7, activation='linear', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 7, activation='linear', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='linear', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbQp8KumfEJB"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1d9tAhvfFF4"
      },
      "source": [
        "hist = model.fit(train_X, train_Y, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K08LIY9GfSnH"
      },
      "source": [
        "print(model.test_on_batch(val_X,val_Y))\n",
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M0acNgQZjjv"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb2Pxs7tbFrV"
      },
      "source": [
        "train_X.shape,train_Y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6pBXRDNagOj"
      },
      "source": [
        "smote = SMOTE('minority')\n",
        "X_sm, y_sm = smote.fit_sample(train_X,train_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yng92wCJbQDF"
      },
      "source": [
        "print(X_sm.shape,y_sm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5wJplVPbeo1"
      },
      "source": [
        "hist = model.fit(X_sm, y_sm, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuuohmNUcAgE"
      },
      "source": [
        "print(model.test_on_batch(val_X,val_Y))\n",
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNepaIljcpj-"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "y_train_labels = np.argmax(train_Y, axis =1)\n",
        "class_weight = class_weight.compute_class_weight('balanced',np.unique(y_train_labels),y_train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZbZz8m1hWEx"
      },
      "source": [
        "hist = model.fit(X_sm, y_sm, batch_size=batch_size, epochs=num_epochs, class_weight=class_weight, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAOvkDdhi0UV"
      },
      "source": [
        "print(model.test_on_batch(val_X,val_Y))\n",
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgn3NL5djKZF"
      },
      "source": [
        "hist = model.fit(train_X, train_Y, batch_size=batch_size, epochs=num_epochs, class_weight=class_weight, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YjSygBHjX1b"
      },
      "source": [
        "print(model.test_on_batch(val_X,val_Y))\n",
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDB3uZSQA5Xu"
      },
      "source": [
        "model.evaluate(val_X,val_Y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}